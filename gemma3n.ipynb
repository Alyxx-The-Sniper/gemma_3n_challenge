{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e69a6f",
   "metadata": {},
   "source": [
    "# Install and Imports\n",
    "This code demostration is a copy from kaggle notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c11830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langgraph\n",
    "# !pip install -q timm==1.0.17\n",
    "# !pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95821879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from typing import Optional, Sequence\n",
    "from langgraph.graph import StateGraph, START, END, add_messages # (we can remove if not needed, we will not using langgraph only the add_message is needed we can mimic that)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fc37a5",
   "metadata": {},
   "source": [
    "# Agent State / Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f82243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"Defines the state of our agent.\"\"\"\n",
    "    audio_path: Optional[str]\n",
    "    image_path: Optional[str] \n",
    "    transcribed_text: Optional[str]\n",
    "    image_description: Optional[str] \n",
    "    news_report: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    current_feedback: Optional[str]\n",
    "\n",
    "    # instruct version\n",
    "    gemma3n_2b_model_path = kagglehub.model_download(\"google/gemma-3n/transformers/gemma-3n-e2b-it\")\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(gemma3n_2b_model_path)\n",
    "    model = AutoModelForImageTextToText.from_pretrained(gemma3n_2b_model_path, torch_dtype=\"auto\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924547d2",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1f6595",
   "metadata": {},
   "source": [
    "### Function helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2057c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function helper to call gemma\n",
    "def generate(messages):\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device, dtype=model.dtype)\n",
    "    \n",
    "    outputs = model.generate(**inputs, max_new_tokens=512, disable_compile=True)\n",
    "    text = processor.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
    "    \n",
    "    # clean-up the variables to free-up GPU RAM\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70102ad",
   "metadata": {},
   "source": [
    "### Transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712719ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample audio\n",
    "import requests\n",
    "from IPython.display import Audio\n",
    "\n",
    "audio_url='https://drive.google.com/uc?export=download&id=1sJrtX0N_Das3LnKOwIMagOrVG7mX9EVG'\n",
    "\n",
    "# Download the audio file\n",
    "response = requests.get(audio_url)\n",
    "with open(\"sample_audio.wav\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Play the downloaded audio file\n",
    "Audio(\"sample_audio.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcribe audio\n",
    "def transcribe_audio_node(state: AgentState) -> dict:\n",
    "    \"\"\"Transcribes the audio file specified in the state.\"\"\"\n",
    "    print(\"--- üé§ TRANSCRIBING AUDIO ---\")\n",
    "    # audio_path = state['audio_path']\n",
    "    audio_path = state.get('audio_path')\n",
    "    \n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"audio\", \"audio\": audio_path},\n",
    "            {\"type\": \"text\", \"text\": \"Transcribe the following audio. Provide only the transcribed text.\"}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    transcribed_text = generate(messages)\n",
    "    print(f\"   > Transcription: {transcribed_text[:800]}...\")\n",
    "    return {\"transcribed_text\": transcribed_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Testing ####\n",
    "test_state = {\n",
    "    \"audio_path\": \"sample_audio.wav\"}\n",
    "\n",
    "# 2. Call the function directly with the test state\n",
    "print(\"--- üß™ TESTING transcribe_audio_node ---\")\n",
    "transcription_result = transcribe_audio_node(test_state)\n",
    "\n",
    "# 3. Print the result to verify the output\n",
    "print(\"\\n--- ‚úÖ TEST COMPLETE ---\")\n",
    "transcription_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420b8c6f",
   "metadata": {},
   "source": [
    "### Describe Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d21a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampel image\n",
    "from IPython.display import Image\n",
    "image_url = 'https://drive.google.com/uc?export=download&id=1-p8xLxUPZzoAwNTUFV29b90N08t6wZfl'\n",
    "Image(url=image_url,height=480,width=480)\n",
    "\n",
    "\n",
    "response = requests.get(image_url)\n",
    "with open(\"river.jpg\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "Image('river.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c536f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_image_tool(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    Takes an image path from the state,\n",
    "    describes the image like a news anchor, reporter, and journalist using a multimodal LLM if an image_path is present.\n",
    "    and returns a dictionary to update the state.\n",
    "    \"\"\"\n",
    "    print(\"--- üñºÔ∏è DESCRIBING IMAGE ---\")\n",
    "    image_path = state.get('image_path')\n",
    "\n",
    "\n",
    "    # Prepare the prompt for the model\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            # The model needs both the image and a text prompt\n",
    "            {\"type\": \"image\", \"image\": image_path},\n",
    "            {\"type\": \"text\", \"text\": 'Describe this image in detail.'}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    # Call the existing helper function to get the description\n",
    "    image_description = generate(messages)\n",
    "    print(f\"   > Description generated successfully.\")\n",
    "\n",
    "    # Return a dictionary with the state field to update\n",
    "    return {\"image_description\": image_description}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdfe18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing ###\n",
    "test_state = {\n",
    "    \"image_path\": 'river.jpg'}\n",
    "\n",
    "print(\"--- üß™ TESTING Decribe Image ---\")\n",
    "image_result = describe_image_tool(test_state)\n",
    "\n",
    "print(\"\\n--- ‚úÖ TEST COMPLETE ---\")\n",
    "image_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d248f76c",
   "metadata": {},
   "source": [
    "### Ai Report Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f53cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "def ai_agent_reporter(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Generates a news report from transcription and/or image description using Gemma-3n.\n",
    "    \"\"\"\n",
    "    print(\"--- ‚úçÔ∏è GENERATING NEWS REPORT ---\")\n",
    "    \n",
    "    # 1. Build the prompt from the state\n",
    "    context_parts = [\n",
    "        \"You are an expert news reporter. Your task is to write a clear, concise, and factual news report based on the following information.\",\n",
    "        \"Synthesize all available information into a single, coherent story. Present it as a professional news report.\"\n",
    "    ]\n",
    "\n",
    "    transcribed_text = state.get('transcribed_text')\n",
    "    image_description = state.get('image_description')\n",
    "\n",
    "    if not transcribed_text and not image_description:\n",
    "        return {\"news_report\": [AIMessage(content=\"No input provided to generate a report.\")]}\n",
    "\n",
    "    if transcribed_text:\n",
    "        context_parts.append(f\"--- Transcribed Audio ---\\n\\\"{transcribed_text}\\\"\")\n",
    "    if image_description:\n",
    "        context_parts.append(f\"--- Image Description ---\\n\\\"{image_description}\\\"\")\n",
    "    \n",
    "    # 2. Call the correct (Gemma-3n) model\n",
    "    prompt = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "\n",
    "    # The 'content' must be a list containing a text dictionary\n",
    "    # to match the multimodal format expected by our 'generate' function.\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    report_content = generate(messages)\n",
    "    print(\"   > Report generated successfully.\")\n",
    "\n",
    "    # 3. Return only the updated part of the state\n",
    "    return {\"news_report\": [AIMessage(content=report_content)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d85853",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing ###\n",
    "# 1. Create a sample state with dummy data\n",
    "test_state = {\n",
    "    \"transcribed_text\": \"The city council today approved the new budget for the fiscal year. The measure passed with a 7-2 vote after a lengthy debate.\",\n",
    "    \"image_description\": \"A photograph shows a group of five officials sitting at a long wooden desk in a formal meeting room. The official in the center is speaking into a microphone.\",\n",
    "    \"news_report\": [] # This is required by the AgentState definition\n",
    "}\n",
    "\n",
    "# 2. Call the corrected function\n",
    "report_result = ai_agent_reporter(test_state)\n",
    "\n",
    "# 3. Print the result\n",
    "print(\"\\n--- ‚úÖ TEST COMPLETE ---\")\n",
    "\n",
    "# You can also inspect the content directly\n",
    "print(\"\\nGenerated Report:\")\n",
    "print(report_result['news_report'][0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1928ca",
   "metadata": {},
   "source": [
    "### Revise Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eec7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revise_report_node(state: dict) -> dict:\n",
    "    \"\"\"Revises the news report based on the latest human feedback using Gemma-3n.\"\"\"\n",
    "    print(\"--- üîÑ REVISING REPORT ---\")\n",
    "    \n",
    "    # Get context from the state\n",
    "    transcribed = state.get(\"transcribed_text\", \"Not available.\")\n",
    "    \n",
    "    # Get the latest human feedback and the last AI report from the message history\n",
    "    human_feedback = state['news_report'][-1].content\n",
    "    last_ai_report = state['news_report'][-2].content\n",
    "\n",
    "    # Construct the prompt for the model\n",
    "    prompt = f\"\"\"You are a professional news editor.\n",
    "Revise the news report to address the feedback. Ensure clarity, grammar, and style are improved, while staying faithful to the original transcription.\n",
    "\n",
    "**Original Transcription:**\n",
    "\"{transcribed}\"\n",
    "\n",
    "**Current Draft of News Report:**\n",
    "\"{last_ai_report}\"\n",
    "\n",
    "**Latest Human Feedback:**\n",
    "\"{human_feedback}\"\n",
    "\n",
    "Provide only the full, revised news report as your response.\n",
    "\"\"\"\n",
    "    \n",
    "    # Format the messages correctly for our multimodal 'generate' function\n",
    "    messages = [{\"role\": \"user\", \n",
    "                 \"content\": [{\"type\": \"text\", \"text\": prompt}]}]\n",
    "    \n",
    "    # Use the 'generate' function for Gemma-3n\n",
    "    revised_content = generate(messages)\n",
    "    print(\"   > Revision complete.\")\n",
    "\n",
    "    # Return the new AI message to be added to the state\n",
    "    return {\"news_report\": [AIMessage(content=revised_content)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20e8bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "# 1. Create a sample state that mimics a conversation history.\n",
    "#    The 'news_report' list must have an AI message followed by a Human message.\n",
    "test_state_for_revision = {\n",
    "\n",
    "    \"transcribed_text\": \"The city council today approved the new budget for the fiscal year.\",\n",
    "    \n",
    "    \"news_report\": [\n",
    "        AIMessage(content=\"The council has approved the new budget for the upcoming fiscal year after a lengthy debate.\"),\n",
    "        HumanMessage(content=\"Revise the report to be in tagalog langguage.\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 2. Call the corrected revision function\n",
    "revision_result = revise_report_node(test_state_for_revision)\n",
    "\n",
    "# 3. Print the result\n",
    "print(\"\\nRevised Report Content:\")\n",
    "print(revision_result['news_report'][0].content)\n",
    "print(\"\\n--- ‚úÖ TEST COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda87e72",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a77095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_report_node(state: dict) -> dict:\n",
    "    \"\"\"Saves the latest AI-generated news report to a text file.\"\"\"\n",
    "    print(\"--- üíæ SAVING REPORT ---\")\n",
    "    \n",
    "    # Find the latest message from the AI\n",
    "    latest_report_msg = next(\n",
    "        (msg for msg in reversed(state[\"news_report\"]) if isinstance(msg, AIMessage)), \n",
    "        None\n",
    "    )\n",
    "    \n",
    "    if not latest_report_msg:\n",
    "        return {\"final_message\": \"Error: No report available to save.\"}\n",
    "\n",
    "    # Prepare to save the file\n",
    "    output_dir = \"saved_reports\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filename = os.path.join(output_dir, \"news_report.txt\")\n",
    "\n",
    "    # Write the content to the file\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(latest_report_msg.content)\n",
    "    \n",
    "    final_message = f\"‚úÖ News report successfully saved to: {filename}\"\n",
    "    print(f\"   > {final_message}\")\n",
    "\n",
    "    # Return a dictionary to update the state\n",
    "    return {\"final_message\": final_message}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a2f9ff",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c916276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== GRADIO APPLICATION LOGIC =====\n",
    "def run_initial_generation(audio_path, image_path):\n",
    "    \"\"\"Handles the first step and returns all necessary outputs for the UI.\"\"\"\n",
    "    if not audio_path and not image_path:\n",
    "        return \"Please provide an audio or image file.\", None, gr.update(visible=False), None, None, None\n",
    "\n",
    "    # Run the pipeline\n",
    "    state = AgentState(audio_path=audio_path, image_path=image_path, news_report=[])\n",
    "    state.update(transcribe_audio_node(state))\n",
    "    state.update(describe_image_tool(state))\n",
    "    state.update(ai_agent_reporter(state))\n",
    "\n",
    "    # Extract info for UI\n",
    "    latest_report = state[\"news_report\"][-1].content\n",
    "    transcribed_text = state.get('transcribed_text') or \"No audio was provided to transcribe.\"\n",
    "    image_description = state.get('image_description') or \"No image was provided to describe.\"\n",
    "\n",
    "    return latest_report, state, gr.update(visible=True), \"\", transcribed_text, image_description\n",
    "\n",
    "def run_revision(feedback, current_state):\n",
    "    \"\"\"Handles revision and ensures all UI fields are correctly populated.\"\"\"\n",
    "    if not feedback or not feedback.strip():\n",
    "        latest_report = next((msg.content for msg in reversed(current_state[\"news_report\"]) if isinstance(msg, AIMessage)), \"\")\n",
    "        transcribed_text = current_state.get('transcribed_text', \"\")\n",
    "        image_description = current_state.get('image_description', \"\")\n",
    "        return latest_report, current_state, \"Please provide feedback.\", transcribed_text, image_description\n",
    "\n",
    "    # Run revision pipeline\n",
    "    current_state[\"news_report\"] = add_messages(current_state[\"news_report\"], [HumanMessage(content=feedback)])\n",
    "    current_state.update(revise_report_node(current_state))\n",
    "\n",
    "    # Extract info for UI\n",
    "    latest_report = current_state[\"news_report\"][-1].content\n",
    "    transcribed_text = current_state.get('transcribed_text') or \"No audio was provided.\"\n",
    "    image_description = current_state.get('image_description') or \"No image was provided.\"\n",
    "\n",
    "    return latest_report, current_state, \"\", transcribed_text, image_description\n",
    "\n",
    "def run_save(current_state):\n",
    "    save_update = save_report_node(current_state)\n",
    "    return save_update[\"final_message\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a9b105",
   "metadata": {},
   "outputs": [],
   "source": [
    " # ===== GRADIO UI DEFINITION =====\n",
    "import gradio as gr\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"Multimodal News Reporter\") as demo:\n",
    "    agent_state = gr.State(value=None)\n",
    "\n",
    "    gr.Markdown(\"# üì∞ Multimodal News Reporter AI\")\n",
    "    gr.Markdown(\"Upload an audio recording and/or a relevant image. The AI will generate a news report that you can then revise and save.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            audio_input = gr.Audio(label=\"Audio Evidence\", type=\"filepath\")\n",
    "            image_input = gr.Image(label=\"Image Evidence\", type=\"filepath\")\n",
    "            generate_btn = gr.Button(\"üìù Generate Initial Report\", variant=\"primary\")\n",
    "        with gr.Column(scale=2):\n",
    "            report_output = gr.Textbox(label=\"Generated News Report\", lines=12, interactive=False)\n",
    "            status_output = gr.Markdown(value=\"\")\n",
    "            \n",
    "            # --- NEW: Collapsible section for source info ---\n",
    "            with gr.Accordion(\"Show Source Information\", open=False) as source_info_accordion:\n",
    "                transcribed_audio_output = gr.Textbox(label=\"üé§ Transcribed Audio\", interactive=False, lines=5)\n",
    "                image_description_output = gr.Textbox(label=\"üñºÔ∏è Image Description\", interactive=False, lines=5)\n",
    "\n",
    "            with gr.Group(visible=False) as revision_group:\n",
    "                gr.Markdown(\"### ‚úçÔ∏è Provide Feedback for Revision\")\n",
    "                feedback_input = gr.Textbox(label=\"Your Feedback\", placeholder=\"e.g., 'Make the tone more formal.'\")\n",
    "                with gr.Row():\n",
    "                    revise_btn = gr.Button(\"üîÑ Revise Report\")\n",
    "                    save_btn = gr.Button(\"üíæ Save Final Report\")\n",
    "\n",
    "    # --- Event Handlers (UPDATED) ---\n",
    "    generate_btn.click(\n",
    "        fn=run_initial_generation,\n",
    "        inputs=[audio_input, image_input],\n",
    "        outputs=[report_output, agent_state, revision_group, status_output, transcribed_audio_output, image_description_output]\n",
    "    )\n",
    "    revise_btn.click(\n",
    "        fn=run_revision,\n",
    "        inputs=[feedback_input, agent_state],\n",
    "        outputs=[report_output, agent_state, status_output, transcribed_audio_output, image_description_output]\n",
    "    ).then(fn=lambda: \"\", outputs=[feedback_input])\n",
    "    save_btn.click(\n",
    "        fn=run_save,\n",
    "        inputs=[agent_state],\n",
    "        outputs=[status_output]\n",
    "    )\n",
    "\n",
    "# 5. ===== LAUNCH THE APP =====\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c8cbb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11829b65",
   "metadata": {},
   "source": [
    "# Write up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7192e0fc",
   "metadata": {},
   "source": [
    "üìù Project Title: Newsly ‚Äì Field Reporting Reinvented\n",
    "üó£Ô∏è Why We Built Newsly\n",
    "‚ÄúIn every storm, every quake, every conflict‚Äî\n",
    "our reporters are there.\n",
    "Not for fame.\n",
    "But to get the truth out.\n",
    "\n",
    "Sometimes, they miss details. They make mistakes.\n",
    "Not because they don‚Äôt care‚Äî\n",
    "but because they‚Äôre doing everything manually.\n",
    "\n",
    "Often, we forget the reporters.\n",
    "No sleep. No food.\n",
    "Calamity strikes, and still‚Äî\n",
    "they stay awake all night and day,\n",
    "just to deliver the truth.\n",
    "Writing, editing, transcribing‚Ä¶ all by hand.\n",
    "\n",
    "We see their hard work.\n",
    "We see the danger they face.\n",
    "\n",
    "Newsly changes that.\n",
    "\n",
    "One tap to transcribe.\n",
    "One photo to describe.\n",
    "One second to report.\n",
    "\n",
    "No signal. No delay. Just the truth.\n",
    "\n",
    "üìå Problem Statement\n",
    "Journalists and field reporters often work in extreme conditions ‚Äî natural disasters, power outages, no signal. Yet they are expected to capture, write, and deliver accurate news, often without sleep, rest, or proper tools.\n",
    "This manual workload increases errors, delays reporting, and contributes to burnout.\n",
    "\n",
    "üöÄ Solution: Newsly App\n",
    "Newsly is a mobile-first AI assistant that helps field reporters transcribe, describe, and generate reports on the fly ‚Äî even offline. Built with a multimodal LLM, it handles audio, images, and text to deliver fast, accurate, editable news drafts anytime, anywhere.\n",
    "\n",
    "‚öôÔ∏è Key Features\n",
    "üé§ Audio Recording + Transcription\n",
    "Record interviews and transcribe them instantly ‚Äî offline.\n",
    "\n",
    "üì∏ Photo Capture + Auto Description\n",
    "Snap photos and generate scene-based descriptions.\n",
    "\n",
    "üß† AI-Powered Report Generation\n",
    "Turn interviews, photos, and field notes into editable reports in seconds.\n",
    "\n",
    "üìù Human Review & Edits\n",
    "Edit headlines, adjust tone, and finalize reports in-app.\n",
    "\n",
    "üì° Works Offline\n",
    "Full functionality without internet access ‚Äî ideal for disaster zones.\n",
    "\n",
    "üë• Target Users\n",
    "Journalists & field reporters\n",
    "\n",
    "Local news teams\n",
    "\n",
    "Emergency response media units\n",
    "\n",
    "NGOs and humanitarian storytellers\n",
    "\n",
    "üíª Technology Stack\n",
    "Multimodal LLM for text, image, and voice understanding\n",
    "\n",
    "Lightweight mobile interface (Android-first)\n",
    "\n",
    "Local-first processing for offline use\n",
    "\n",
    "Supports Tagalog and English transcription and generation\n",
    "\n",
    "üåç Impact\n",
    "Newsly doesn‚Äôt replace the reporter ‚Äî it empowers them.\n",
    "It reduces cognitive load, prevents burnout, and speeds up truth-telling during high-stakes situations.\n",
    "With Newsly, reporters stay focused on the story ‚Äî not the typing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10539b3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma3n-challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
